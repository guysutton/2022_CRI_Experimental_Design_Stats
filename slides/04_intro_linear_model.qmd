---
title: "Session #4: Introduction to linear models"
author: "Guy F. Sutton"
institute: "Centre for Biological Control<br>Rhodes University, South Africa<Br>Email: g.sutton@ru.ac.za"
format: 
  revealjs:
    # incremental: true
    logo: images/cbc_logo_full.jpg
    # footer: "`emitanaka.org/slides/toronto2022`"
    slide-number: true
    chalkboard: 
      boardmarker-width: 5
    # css: [assets/syntax-highlight.css, assets/custom.css, assets/pacman.css]
    # multiplex: true
    # controls: true
    # theme: [simple, assets/monash.scss]
    # header-includes: |
    #   <link rel="stylesheet" href="/assets/fontawesome-free-6.1.1-web/css/font-awesome.min.css">
    #   <script defer src="/assets/fontawesome-free-6.1.1-web/js/all.min.js"></script>
editor: source
---

```{r}
#| echo: false
#| eval: true

################
# Session setup 
################

# Use package manager to check, install and load required packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  # List the packages you need below
  tidyverse
)

# Change ggplot theme
theme_set(
  theme_classic() +
    theme(
      panel.border = element_rect(colour = "black",
                                  fill = NA),
      axis.text = element_text(colour = "black"),
      axis.title.x = element_text(margin = unit(c(2, 0, 0, 0),
                                                "mm")),
      axis.title.y = element_text(margin = unit(c(0, 4, 0, 0),
                                                "mm")),
      legend.position = "none"
    )
)
```

## Linear models 

::: {.incremental}
- Have you heard of `t-test`, `ANOVA`, `linear regression`, `ANCOVA`, ect??? 
    + These are all [linear models]{style="color:#0000FF"}
    + In fact, these models all fall under the umbrella of a [General Linear Model (GLM)]{style="color:#0000FF"}
      + So, I want you to forget about the `t-test`, `ANOVA`, `linear regression`, `ANCOVA`...
      + All we will be discussing in this course are [General Linear Model's (GLMs)]{style="color:#0000FF"}
::: 

---

## How do they work? 

::: {.incremental}
- They assume a linear relationship between our predictor variable(s) (`x`) and our response variable (`y`)
:::

. . . 

::: columns
::: {.column width="40%"}
```{r}
#| echo: false
#| eval: true

#####################################
# Simulate data to fit a linear model
#####################################

# Set reproducible seed 
set.seed(1234)

# Generate simulated data (continuous x variable)
data <- tibble(
  x1 = rnorm(n = 100, mean = 20, sd = 5),
  error = rnorm(n = 100, mean = 0, sd = 5),
  y = 100 + (3 * x1) + error
  ) %>%
  dplyr::select(
    fruit_diameter = x1, 
    fruit_mass = y
  )
```

```{r}
#| echo: false
#| eval: true

head(data, 10)
```
:::

::: {.column width="60%"}
```{r}
#| echo: false
#| eval: true
#| fig-height: 8
#| fig-width: 11

data %>%
  ggplot(data = ., aes(x = fruit_diameter, 
                       y = fruit_mass)) +
  geom_point() + 
  theme(axis.title.x = element_text(size = 30),
        axis.title.y = element_text(size = 30))
```
:::
:::

---

## How do they work? 

::: {.incremental}
- Very simply, we are asking the model to determine if we are better able to predict `y`, knowing the information we do about our `x` variable(s) than if the model didn't have this information. 
  + E.g. Can we better predict `fruit_mass` knowing the `fruit_diameter` than if we didn't have any information about `fruit_diameter`? 
:::

---

## Model with a single numeric predictor variable

- Fit a model that looks at the relationship between `fruit_mass` and `fruit_diameter`? 

. . . 

::: {style="font-size: 0.65em"}
```{r}
#| echo: true
#| eval: true
mod_global <- glm(fruit_mass ~ 1 + fruit_diameter, data = data)
summary(mod_global)
```
:::

---

## Fit a null model 

::: {.incremental}
- But, how do we know if our ability to predict `fruit_mass` from `fruit_diameter` is better than random chance alone? 
  + Hint: We need to fit another model 
:::

. . . 

::: {style="font-size: 0.60em"}
```{r}
#| echo: true
#| eval: true
mod_null <- glm(fruit_mass ~ 1, data = data)
summary(mod_null)
```
:::

---

## What is a null model? 

::: {.incremental}
::: {style="font-size: 0.85em"}
- [Null model]{style="color:#0000FF"} represents the null hypothesis (H~0~).
  + The null hypothesis is that there is no evidence for a statistically significant effect/relationship of our predictor variable(s) (`x`) on our response variable `y`.

- What is the null hypothesis in our `fruit_mass` and `fruit_diameter` example?
    + H~0~: There is no evidence for a statistically significant relationship between `fruit_mass` and `fruit_diameter`.
    + When we only have 1 predictor variable in our model, the null hypothesis is basically a representation of *random chance*. 

- The [null model]{style="color:#0000FF"} is the [global model]{style="color:#FF0000"} *minus* the predictor variable that you want to test. 
:::
:::

---

## What is a global model? 

::: {.incremental}
::: {style="font-size: 0.85em"}
- [Global model]{style="color:#FF0000"} represents the alternative hypothesis (H~1~).
  + The alternative hypothesis is that there is evidence for a statistically significant effect/relationship of our predictor variable(s) (`x`) on our response variable `y`.

- What is the alternative hypothesis in our `fruit_mass` and `fruit_diameter` example?
    + H~1~: There is evidence for a statistically significant relationship between `fruit_mass` and `fruit_diameter`.

- The [global model]{style="color:#FF0000"} model is how we represent the alternative hypothesis, and thus, is a model that **includes** the predictor variable that you want to test. 
:::
:::

---

## Piecing it together 

Can we better predict `fruit_mass` knowing the `fruit_diameter` than if we didn't have any information about `fruit_diameter`? 

. . . 

1. [Global model (H~1~)]{style="color:#FF0000"}: There is a relationship between``fruit_mass` and `fruit_diameter`. 

```{r}
#| echo: true
#| eval: true

mod_global <- glm(fruit_mass ~ 1 + fruit_diameter, data = data)
```

. . . 

2. [Null model (H~0~)]{style="color:#0000FF"}: There is no statistical evidence for a relationship between``fruit_mass` and `fruit_diameter`. 

```{r}
#| echo: true
#| eval: true

mod_null <- glm(fruit_mass ~ 1, data = data)
```

---

## Hypothesis test 

::: {.incremental}
- Finally, we have to actually perform a hypothesis test
  + Was the [global model (H~1~)]{style="color:#FF0000"} or the [null model (H~0~)]{style="color:#0000FF"} better supported by the data?
  
- To do this, we use the **Likelihood Ratio Test (LRT)**
  + In `R`, we perform the LRT using the following code:
  + `anova(null_model, global_model, test = "Chisq")`
:::

---

## Likelihood Ratio Test (LRT)

- This test gives us our test statistic (), degrees of freedom (df), and our sacred p-value. 

```{r}
#| echo: true
#| eval: true

anova(mod_null, mod_global, test = "Chisq")
```

---

## Reporting LRT 

```{r}
#| echo: true
#| eval: true

anova(mod_null, mod_global, test = "Chisq")
```

::: {.incremental}
::: {style="font-size: 0.70em"}
- There is a statistically significant relationship between `fruit_diameter` and `fruit_mass` (X2 = 22083, df = 1, *P* < 0.001).
  + We know that one of the models was better than the other because the *P* < 0.05.
  + We then can tell that model 2 (`mod_global`) is the better model because it has a **lower** residual deviance (`Resid. Dev = 2635.2`) than model 1 (`mod_null`) (`Resid. Dev = 24717.8`). 
    + Residual deviance shows how well the model can predict `y`, **more deviance = poorer accuracy!!!**
:::
:::

---

## Model inference for a single numeric `x` variable 

::: {style="font-size: 0.45em"}
```{r}
#| echo: true
#| eval: true

summary(mod_global)
```
:::

. . .

::: {.incremental}
::: {style="font-size: 0.55em"}
- We can derive some cool information from our fitted model.
  + The `Estimate` for the `(Intercept)` column = the mean of fruit mass when x = 0.
    + This is obviously nonsense for our model, as we can't have fruit_diameter = 0. 
  + The `Estimate` for the `fruit_diameter` column gives us the beta coefficient ($\beta$~1~),
  which tells us the change in fruit_mass (`y`) for every 1 unit change in fruit diameter (`x`).
:::
:::

---

## Beta co-efficient 

::: {style="font-size: 0.55em"}
```{r}
#| echo: true
#| eval: true

summary(mod_global)
```
:::

. . . 

::: {.incremental}
::: {style="font-size: 0.65em"}
- The `Estimate` for the `fruit_diameter` column gives us the beta coefficient ($\beta$~1~), which tells us the change in fruit_mass (`y`) for every 1 unit change in fruit diameter (`x`).
  + In this example, $\beta$~1~ = 2.9739:
    + This tells us that for every 1 mm change in `fruit_diameter`, `fruit_mass` increases by about 2.93 grams, on average. 
:::
:::

---

## Model syntax 

- To fit a statistical model in `R`, the syntax is quite consistent. 

. . . 

::: {.incremental}
- `statistical_model(`
   + `response_variable ~ predictor_variable(s),`
   + `data = data,`
   + `family = type of statistical distribution`
   + `)`
:::

---

## Model syntax - model formula

- To fit a [GLM]{style="color:#0000FF"}, we use the built-in `glm` function. 

. . . 

`glm()`

---

## Model syntax - response (`y`) variable 

- Next, we specify our the column name of our response variable (`y`)

. . . 

`glm(y)`

--- 

## Model syntax - formula

- Next, we use the tilde (`~`) symbol to specify that we want anything on the left to be our response variable (`y`), and anything to the right to be our predictor variable(s) (`x1`, `x2`). 

. . . 

`glm(y ~ )`

--- 

## Model syntax - predictor (`x`) predictors 

::: {.incremental}
- Next, we use specify the column names of our predictor variable(s) (`x1`, `x2`).
  + Remember, these are our *treatment* variables. 

- We can have a single predictor (`x`)variable
  + `glm(y ~ x1)`
- We can have multiple predictor variables (e.g. `x1` and `x2`)
  + `glm(y ~ x1 + x2)`
:::

---

## Model syntax - dataset

::: {.incremental}
- Next, we have to specify the name of our dataset. 
  + I.e. Where can it find the column names containing the response and predictor variable(s) 
  + e.g. If our data is stored in a variable called `df` 
:::

. . . 

`glm(y ~ x1, data = df)`

---

## Model syntax - dataset

::: {.incremental}
- Next, we have to specify the statistical distribution we want to fit.
  + There will be more on this in later sessions. 
  + If we want a normal/Gaussian distribution (e.g. we can leave this argument blank), or specify the distribution specifically. 
:::

. . . 

`glm(y ~ x1, data = df, family = gaussian(link = "identity"))`

---

## Model syntax - saving the model 

::: {.incremental}
- Lastly, we usually want to store the fitted statistical model into a new variable so that we can access the results and perform additional on the fitted model. 
  + E.g. Let's store our fitted model in a new variable called `mod1`
:::

. . . 

`mod1 <- glm(y ~ x1, data = df, family = gaussian(link = "identity"))`

---

## Model with a single categorical predictor variable

::: columns
::: {.column width="40%"}
```{r}
#| echo: false
#| eval: true

#####################################
# Simulate data to fit a linear model
#####################################

# Set reproducible seed 
set.seed(1234)

# Generate simulated data (categorical x variable)
data <- tibble(
  x1 = rep(letters[1:2], length.out = 30),
  y = rnorm(n = 30, mean = c(20, 30), sd = c(5, 8)),
  ) %>%
  dplyr::select(
    farm = x1, 
    nitrogen_c = y
  ) %>%
  dplyr::mutate(farm = dplyr::if_else(farm == "a", "Farm A", "Farm B"))
```

```{r}
#| echo: false
#| eval: true

head(data, 10)
```
:::

::: {.column width="60%"}
```{r}
#| echo: false
#| eval: true
#| fig-height: 8
#| fig-width: 11

data %>%
  ggplot(data = ., aes(x = farm, 
                       y = nitrogen_c)) +
  geom_boxplot() + 
  theme(axis.title.x = element_text(size = 30),
        axis.title.y = element_text(size = 30))
```
:::
:::

---

## How do they work? 

::: {.incremental}
- Models with a categorical predictor work very similarly to the model we just worked through with a continuous predictor variable. 
  + E.g. Can we better predict `nitrogen_c` knowing the `farm` the sample was taken from than if we didn't have any information about `farm`? 
  + I.e. Does `nitrogen_c` differ depending on the `farm` the sample was taken from? 
:::

---

## Fit model with categorical predictor 

1. [Global model (H~1~)]{style="color:#FF0000"}: There is statistical evidence that `nitrogen_c` differs between the two `farm`'s we sampled. 

```{r}
#| echo: true
#| eval: true

mod_global <- glm(nitrogen_c ~ 1 + farm, data = data)
```

. . . 

<br>

2. [Null model (H~0~)]{style="color:#0000FF"}: There is no evidence that `nitrogen_c` differs between the two `farm`'s we sampled. 

```{r}
#| echo: true
#| eval: true

mod_null <- glm(nitrogen_c ~ 1, data = data)
```

---

## Hypothesis test - LRT

```{r}
#| echo: true
#| eval: true

anova(mod_null, mod_global, test = "Chisq")
```

::: {.incremental}
::: {style="font-size: 0.70em"}
- There is a statistically significant difference in `nitrogen_c` between the two farms that were sampled (X2 = 269.7, df = 1, *P* < 0.001).
  + We know that one of the models was better than the other because the *P* < 0.05.
  + We then can tell that model 2 (`mod_global`) is the better model because it has a **lower** residual deviance (`Resid. Dev = 775.37`) than model 1 (`mod_null`) (`Resid. Dev = 1045.1`). 
:::
:::

---

## Model inference for a single categorical `x` variable 

::: {style="font-size: 0.45em"}
```{r}
#| echo: true
#| eval: true

summary(mod_global)
```
:::

. . .

::: {.incremental}
::: {style="font-size: 0.55em"}
- Notice that `FarmA` is missing? 
  + This is because `R` is effectively using the `(Intercept)` row as `FarmA`. 
  + The `Estimate` for the `(Intercept)` row = the mean of `nitrogen_c` at `FarmA`. 
    + Unlike the model with the numeric predictor, this comparison actually makes sense. 
  + The `Estimate` for the `FarmB` row gives us the beta coefficient ($\beta$~1~),
  which tells us average difference in `nitrogen_c` between `FarmA` and `FarmB`. 
:::
:::

---

## Beta co-efficient 

::: {style="font-size: 0.60em"}
```{r}
#| echo: true
#| eval: true

summary(mod_global)
```
:::

. . . 

::: {.incremental}
::: {style="font-size: 0.70em"}
- The `Estimate` for the `FarmB` row gives us the beta coefficient ($\beta$~1~),
  which tells us average difference in `nitrogen_c` between `FarmA` and `FarmB`.
  + In this example, $\beta$~1~ = 5.997:
    + This tells us that `nitrogen_c` is 5.99 units higher, on average, at `FarmB` than at `FarmA`. 
:::
:::

